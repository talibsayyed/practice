{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "import openai\n",
    "\n",
    "endpoint = \"https://di-document-prod.cognitiveservices.azure.com/\"\n",
    "az_key = \"4aeec35765f342a7a29bf6044bccd31d\"\n",
    "\n",
    "openai.api_key = \"sk-svcacct-WqOWrSDc4rSGBDG7-QcN4cZqh6SiGuIEPb44EL3_nhxAaoeeMpOhTLbbhA8CrwT3BlbkFJ1RwNoTMV9rjb57Ir5WtfPhMB54f7MKYGjaPgLdsPAHTplF-u-6m3Gb0R-_3-0A\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "#------ Paragraph Class ----------------\n",
    "class Paragraph:\n",
    "    \"\"\"\n",
    "    Class to store all Paragraphs present in a document\n",
    "    \"\"\"    \n",
    "       \n",
    "    def __init__(self,data :str, coordinates:dict):       \n",
    "        self.content = data\n",
    "        self.content_type = \"Paragraph\"\n",
    "        self.content_coordinates = coordinates         \n",
    "#------------------------------------------\n",
    "\n",
    "# ------ Table Class -----------------------\n",
    "class Table:\n",
    "    \"\"\"\n",
    "    Class to store all Tables present in a document. All tables are converted to string inorder to send the table to GPT\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cell_data:list, coordinates:dict) -> None:\n",
    "        self.content = \"\"\"Below is a table in the form of double pipe seperated values denoted by triple $ signs\n",
    "$$$\n",
    "#-TABLE-#\n",
    "$$$\n",
    "        \"\"\"\n",
    "        self.content_type = \"Table\" #Variable that stores type of content extracted from document\n",
    "        self.table_info = \"  \" #Variable that store the table in a string format\n",
    "        self.content_coordinates = coordinates #Variable that store the location of the extracted table\n",
    "        self.process_cells(cell_data)\n",
    "        \n",
    "        \n",
    "    def process_cells(self,cell_data: list)->None:\n",
    "        \"\"\"\n",
    "        Function that processes the table and stores it in a string format\n",
    "\n",
    "        Inputs:\n",
    "            cell_data: Its a list that stores all the cell related information extracted using Azure Document Intelligence service\n",
    "\n",
    "        Output:\n",
    "            None \n",
    "        \"\"\"\n",
    "        \n",
    "        # Fetching cell Header\n",
    "        for cell in cell_data:\n",
    "            if cell.kind == 'columnHeader':\n",
    "                self.table_info += cell.content.replace(\"\\n\", \"\")+\" || \"\n",
    "                \n",
    "        self.table_info += '\\n'\n",
    "        \n",
    "        #Fetching cell content\n",
    "        row_index=0\n",
    "        for index in range(len(cell_data)):\n",
    "                if cell_data[index].kind == 'content':\n",
    "                    if cell_data[index].row_index != row_index:\n",
    "                        data = cell_data[index].content.replace('\\n', '')\n",
    "                        self.table_info += f\"\\n {row_index+1}) {data}\"\n",
    "                        row_index=cell_data[index].row_index\n",
    "                    else:\n",
    "                        self.table_info += f\" || {cell_data[index].content}\" \n",
    "                        \n",
    "        self.content = self.content.replace(\"#-TABLE-#\", self.table_info+\"\\n\")\n",
    "#----------------------------------------------\n",
    "\n",
    "\n",
    "#--------- Page Class -------------------------\n",
    "class Page:\n",
    "    \"\"\"\n",
    "    Class to store all the contents that a page has in a page object\n",
    "    \"\"\"\n",
    "    def __init__(self, page_number):\n",
    "        self.page_number = page_number\n",
    "        self.content_list = []\n",
    "        self.content_dataframe = pd.DataFrame()\n",
    "        \n",
    "    def convert_to_dataframe(self)->None:\n",
    "        \"\"\"\n",
    "        Function to convert a list of contents into a dataframe\n",
    "        \"\"\"\n",
    "        self.content_dataframe = pd.concat(self.content_list, ignore_index=True)\n",
    "        self.content_dataframe.reset_index(inplace=True, drop=True)\n",
    "#----------------------------------------------\n",
    "\n",
    "\n",
    "#--------- Document Class -----------------\n",
    "class Document:\n",
    "    \"\"\"\n",
    "    Class that store all the page details in a document object\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        self.page_list = []\n",
    "\n",
    "    def add_page(self, page:Page)->None:\n",
    "        \"\"\"\n",
    "        Function to add a new page object to page_list variable when a perticular page is processed\n",
    "        \"\"\"\n",
    "        self.page_list.append(page)\n",
    "#-------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import traceback\n",
    "\n",
    "import pandas as pd\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "\n",
    "\n",
    "class DocumentParser:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "        \n",
    "    #----------------------------------------------------\n",
    "    def overlap_percentage(self, box1:list, box2:list)->float:\n",
    "        \"\"\"\n",
    "        Function that calculates overlaps percentage between 2 boxes\n",
    "\n",
    "        Inputs:\n",
    "            box1: A List that consists of coordinates of X0,Y0,X2,Y2\n",
    "            box2: A List that consists of coordinates of X0,Y0,X2,Y2\n",
    "        \n",
    "        Output:\n",
    "            overlap_percentage: Float value that measures the overlap percentage\n",
    "        \"\"\"\n",
    "        # box = [x1, y1, x2, y2], where (x1, y1) and (x2, y2) are opposite corners of the bounding box\n",
    "\n",
    "        # Calculate the intersection rectangle\n",
    "        x_overlap = max(0, min(box1[2], box2[2]) - max(box1[0], box2[0]))\n",
    "        y_overlap = max(0, min(box1[3], box2[3]) - max(box1[1], box2[1]))\n",
    "\n",
    "        # Calculate the areas of the two bounding boxes and the intersection\n",
    "        area_box1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "        area_box2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "        intersection_area = x_overlap * y_overlap\n",
    "\n",
    "        # Calculate the overlap percentage\n",
    "        overlap_percentage = (intersection_area / min(area_box1, area_box2)) * 100\n",
    "\n",
    "        return overlap_percentage\n",
    "    #----------------------------------------------------\n",
    "        \n",
    "    def parse_document(self, azure_analyse_result : DocumentAnalysisClient):\n",
    "        \n",
    "        if azure_analyse_result!=None:\n",
    "            #Getting unique pages\n",
    "            unique_page_numbers = set(region.page_number for paragraph in azure_analyse_result.paragraphs for region in paragraph.bounding_regions)\n",
    "            \n",
    "            #Total Page count\n",
    "            total_pages = len(unique_page_numbers)\n",
    "            \n",
    "            #New Document Object\n",
    "            doc = Document()\n",
    "            for index in range(total_pages):\n",
    "                doc.page_list.append(Page(page_number = index+1))\n",
    "                \n",
    "                \n",
    "            #Fetching per page Paragraphs\n",
    "            #----------------------------------------------------------------------\n",
    "            for para in azure_analyse_result.paragraphs:\n",
    "                \n",
    "                coordinates =  {\n",
    "                    \n",
    "                    \"X0\" : para.bounding_regions[0].polygon[0].x,\n",
    "                    \"Y0\" : para.bounding_regions[0].polygon[0].y,\n",
    "                    \"X1\" : para.bounding_regions[0].polygon[1].x,\n",
    "                    \"Y1\" : para.bounding_regions[0].polygon[1].y,\n",
    "                    \"X2\" : para.bounding_regions[0].polygon[2].x,\n",
    "                    \"Y2\" : para.bounding_regions[0].polygon[2].y,\n",
    "                    \"X3\" : para.bounding_regions[0].polygon[3].x,\n",
    "                    \"Y3\" : para.bounding_regions[0].polygon[3].y,     \n",
    "                }\n",
    "                \n",
    "                paragraph_object = Paragraph(data=para.content, coordinates=coordinates)\n",
    "                paragraph_page_index = para.bounding_regions[0].page_number - 1 \n",
    "                doc.page_list[paragraph_page_index].content_list.append(\n",
    "                    pd.DataFrame(\n",
    "                        {\n",
    "                            \"object\":[paragraph_object],\n",
    "                            \"content_type\":[paragraph_object.content_type],\n",
    "                            \"X0\":[paragraph_object.content_coordinates[\"X0\"]],\n",
    "                            \"Y0\":[paragraph_object.content_coordinates[\"Y0\"]],\n",
    "                            \"X1\":[paragraph_object.content_coordinates[\"X1\"]],\n",
    "                            \"Y1\":[paragraph_object.content_coordinates[\"Y1\"]],\n",
    "                            \"X2\":[paragraph_object.content_coordinates[\"X2\"]],\n",
    "                            \"Y2\":[paragraph_object.content_coordinates[\"Y2\"]],\n",
    "                            \"X3\":[paragraph_object.content_coordinates[\"X3\"]],\n",
    "                            \"Y3\":[paragraph_object.content_coordinates[\"Y3\"]],\n",
    "                        }\n",
    "                    )\n",
    "                )\n",
    "                        \n",
    "            #Fetching per page tables\n",
    "            #------------------------------------------------------------------------\n",
    "            for table in azure_analyse_result.tables:\n",
    "                coordinates = {\n",
    "                    \n",
    "                    \"X0\" : table.bounding_regions[0].polygon[0].x,\n",
    "                    \"Y0\" : table.bounding_regions[0].polygon[0].y,\n",
    "                    \"X1\" : table.bounding_regions[0].polygon[1].x,\n",
    "                    \"Y1\" : table.bounding_regions[0].polygon[1].y,\n",
    "                    \"X2\" : table.bounding_regions[0].polygon[2].x,\n",
    "                    \"Y2\" : table.bounding_regions[0].polygon[2].y,\n",
    "                    \"X3\" : table.bounding_regions[0].polygon[3].x,\n",
    "                    \"Y3\" : table.bounding_regions[0].polygon[3].y, \n",
    "                }\n",
    "                \n",
    "                table_object = Table(cell_data=table.cells, coordinates=coordinates)\n",
    "                table_page_index = table.bounding_regions[0].page_number - 1\n",
    "                \n",
    "                doc.page_list[table_page_index].content_list.append(\n",
    "                    pd.DataFrame(\n",
    "                        {\n",
    "                            \"object\":[table_object],\n",
    "                            \"content_type\":[table_object.content_type],\n",
    "                            \"X0\":[table_object.content_coordinates[\"X0\"]],\n",
    "                            \"Y0\":[table_object.content_coordinates[\"Y0\"]],\n",
    "                            \"X1\":[table_object.content_coordinates[\"X1\"]],\n",
    "                            \"Y1\":[table_object.content_coordinates[\"Y1\"]],\n",
    "                            \"X2\":[table_object.content_coordinates[\"X2\"]],\n",
    "                            \"Y2\":[table_object.content_coordinates[\"Y2\"]],\n",
    "                            \"X3\":[table_object.content_coordinates[\"X3\"]],\n",
    "                            \"Y3\":[table_object.content_coordinates[\"Y3\"]],\n",
    "                        }\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "            #------------------------------------------------------------------------ \n",
    "            \n",
    "                \n",
    "            for index in range(total_pages):\n",
    "                doc.page_list[index].convert_to_dataframe()\n",
    "                \n",
    "            for index in range(len(doc.page_list)):\n",
    "            \n",
    "                per_page = doc.page_list[index]\n",
    "                page_df = per_page.content_dataframe #Getting per page dataframe\n",
    "                table_count = page_df.loc[page_df['content_type']=='Table',:].shape[0] #Counting how many tables are there per page\n",
    "                \n",
    "                if table_count!=0:\n",
    "                    \n",
    "                    table_df = page_df.loc[page_df['content_type']=='Table',:]# Table dataframe\n",
    "                    paragraph_df = page_df.loc[page_df['content_type']=='Paragraph',:]# Paragraph dataframe\n",
    "                    indexes_to_drop = []\n",
    "                    \n",
    "                    for idx, row in table_df.iterrows():\n",
    "                        \n",
    "                        table_coords = [ row['X0'], row['Y0'], row['X2'], row['Y2'] ]\n",
    "                        # Creating new percentage column\n",
    "                        paragraph_df['overlap_percentage'] = paragraph_df.apply(lambda row_para: self.overlap_percentage([row_para[\"X0\"], row_para[\"Y0\"], row_para[\"X2\"], row_para[\"Y2\"]], table_coords), axis=1)\n",
    "                        \n",
    "                        indices_to_drop = paragraph_df[(paragraph_df['overlap_percentage'] > 20)].index            \n",
    "                        paragraph_df.drop(indices_to_drop, inplace=True)# Drop the rows from paragraph_rows\n",
    "                        indexes_to_drop.extend(indices_to_drop.tolist())\n",
    "                    \n",
    "                    doc.page_list[index].content_dataframe.drop(indexes_to_drop, inplace=True)\n",
    "                    indexes_to_drop = []\n",
    "                    doc.page_list[index].content_dataframe.sort_values(by='Y0', inplace=True)\n",
    "\n",
    "            \n",
    "            # Looping over per page\n",
    "            doc_text_list = [] # a list to store per page text\n",
    "            for index in range(len(doc.page_list)):\n",
    "                \n",
    "                logging.info(f\"started processing pageno:- {index+1}\")\n",
    "                data = None\n",
    "                per_page = doc.page_list[index]\n",
    "                data_text = \"\\n\"\n",
    "                # looping over per content in a page\n",
    "                for idx,row in per_page.content_dataframe.iterrows():\n",
    "                    \n",
    "                    data_text += row['object'].content + \"\\n\"\n",
    "                \n",
    "                # adding data\n",
    "                doc_text_list.append(data_text)\n",
    "\n",
    "            return doc_text_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'D:\\E\\vat_table_extraction\\MVF-Test-Files\\02.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started\n"
     ]
    }
   ],
   "source": [
    "def azure_data_extraction(file_path,endpoint,key):\n",
    "    document_analysis_client = DocumentAnalysisClient(\n",
    "    endpoint=endpoint, credential=AzureKeyCredential(az_key))\n",
    "    print(\"Started\")\n",
    "\n",
    "    # Sending file for Data Extraction to Azure\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        poller = document_analysis_client.begin_analyze_document(\n",
    "        \"prebuilt-invoice\", document=f\n",
    "    )\n",
    "\n",
    "    result = poller.result()\n",
    "\n",
    "    return result,result.key_value_pairs, result.tables\n",
    "\n",
    "res,invoice_data,invoice_tabels = azure_data_extraction(file_path,endpoint,az_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_21868\\1098685342.py:150: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  paragraph_df['overlap_percentage'] = paragraph_df.apply(lambda row_para: self.overlap_percentage([row_para[\"X0\"], row_para[\"Y0\"], row_para[\"X2\"], row_para[\"Y2\"]], table_coords), axis=1)\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_21868\\1098685342.py:153: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  paragraph_df.drop(indices_to_drop, inplace=True)# Drop the rows from paragraph_rows\n"
     ]
    }
   ],
   "source": [
    "text_azure_data = DocumentParser().parse_document(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2782"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_azure_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "RecursiveCharacterTextSplitter.split_text() takes 2 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m chunk_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[0;32m      6\u001b[0m chunk_overlap \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[1;32m----> 8\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_text\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_azure_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_overlap\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: RecursiveCharacterTextSplitter.split_text() takes 2 positional arguments but 4 were given"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\"\\\\nRoger Skinner Limited The Mills Stradbroke Eye Suffolk IP21 5HL\\\\nEST. 1688\\\\nSKINNER\\'S\\\\nPhone: 01379 384247 Fax: 01379 388143 Email: info@skinnerspetfoods.co.uk Website: www.skinnerspetfoods.co.uk VAT Reg No. : 291 8176 74 Company Reg. No. : 1272854\\\\nBORN TO BE OUTDOORS\\\\nINVOICE\\\\nYour VAT No. GB 143 2150 14\\\\nInvoice Address:\\\\nDelivery Address:\\\\nMole Valley Farmers Ltd. Holsworthy 6\\\\nMole Valley Farmers Ltd. Holsworthy 6\\\\nExmoor House\\\\nUnderlane\\\\nLime Way\\\\nHolsworthy\\\\nPathfields Business Park\\\\nDevon\\\\nSouth Molton Devon\\\\nEX22 6BL\\\\nEX36 3LH\\\\nBelow is a table in the form of double pipe seperated values denoted by triple $ signs\\\\n$$$\\\\n  Your Ref || Account || Our Ref || Type || Date || Number || \\\\n\\\\n 1) PO 200366454 || MOL06 || 386112 || INV Page: 1 || 21/12/2022 || 315406\\\\n\\\\n$$$\\\\n        \\\\nBelow is a table in the form of double pipe seperated values denoted by triple $ signs\\\\n$$$\\\\n  Product || Description || Unit || Quantity || Price £ || Line Total £ * || \\\\n\\\\n 1) FTJ25 || F&T Junior 2.5kg || 2.50 KG || 4.00 || 5.14 || 20.56 0\\\\n 2) FTJ15 || F&T Junior 15kg || 15.00 KG || 2.00 || 23.00 || 46.00 0\\\\n 3) FTLS15 || Field and Trial Light and Senior 15kg || 15.00 KG || 2.00 || 25.31 || 50.62 0\\\\n 4) FTLS25 || Field and Trial Light and Senior 2.5kg || 2.50 KG || 4.00 || 5.45 || 21.80 0\\\\n 5) FTM25 || Field and Trial Muesli 2.5kg || 2.50 KG || 4.00 || 4.48 || 17.92 0\\\\n 6) FT26 || Field & Trial Working 26 15kg || 15.00 KG || 5.00 || 16.06 || 80.30 0\\\\n 7) FTM15 || Field & Trial Muesli 15kg || 15.00 KG || 12.00 || 17.61 || 211.32 0\\\\n 8) FT18MA || Field & Trial Maintenance 15kg || 15.00 KG || 13.00 || 17.61 || 228.93 0\\\\n 9) DU15 || Field & Trial Duck & Rice 15kg || 15.00 KG || 1.00 || 28.00 || 28.00 0\\\\n 10) FTP15 || Field & Trial Puppy 15kg || 15.00 KG || 3.00 || 30.23 || 90.69 0\\\\n 11) FTP25 || Field & Trial Puppy 2.5kg || 2.50 KG || 4.00 || 6.40 || 25.60 0\\\\n 12) SAL25 || Field & Trial Salmon & Rice 2.5kg || 2.50 KG || 4.00 || 6.40 || 25.60 0\\\\n 13) RR15 || Skinners Ruff & Ready || 15.00 KG || 6.00 || 16.62 || 99.72 1\\\\n 14) TUR15 || Field & Trial Turkey & Rice 15kg || 15.00 KG || 1.00 || 24.79 || 24.79 0\\\\n 15) TUR25 || Field & Trial Turkey & Rice 2.5kg || 2.50 KG || 4.00 || 5.65 || 22.60 0\\\\n\\\\n$$$\\\\n        \\\\n£VAT Analysis\\\\nBelow is a table in the form of double pipe seperated values denoted by triple $ signs\\\\n$$$\\\\n  \\\\n || Gross Amount £ || 994.45\\\\n 1) Discount £ || 0.00\\\\n 2) Line Total £ || 994.45\\\\n 3) VAT £ || 19.94\\\\n 4) Total £ || 1014.39\\\\n\\\\n$$$\\\\n        \\\\nVAT\\\\nRate\\\\nSupplies\\\\nCode\\\\n894.73\\\\n0.00\\\\n0\\\\n0.00\\\\n1\\\\n99.72\\\\n19.94\\\\n20.00\\\\nE & O.E\\\\nPAYMENT 90 DAYS FROM INVOICE DATE. PAYMENT DETAILS: NATWEST - SORT CODE: 52-30-31 - ACCOUNT NUMBER: 45551367 ALL GOODS REMAIN THE PROPERTY OF THE SELLER UNTIL FULL PAYMENT HAS BEEN MADE OF ALL OUTSTANDING MONIES DUE WITH CLEARED FUNDS. FULL TERMS & CONDITIONS OF SALES AVAILABLE UPON REQUEST\\\\n\"]'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
